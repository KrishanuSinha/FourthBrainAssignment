{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971748d6",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=‚Äùfalse‚Äù ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82694370",
   "metadata": {},
   "source": [
    "# üõçÔ∏è Launch New Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dde4c6",
   "metadata": {},
   "source": [
    "Today you are a machine learning engineer at the Department of New Products at Target Cosmetics! \n",
    "\n",
    "We will start with a small dataset on interactions between users and current products from the past and try to discover substructure, if there's any, by applying some **unsupervised learning** methods. \n",
    "\n",
    "Then we will leverage the small amount of labeled data (current products) in combination with a larger amount of unlabeled data (new products to launch) to make estimations as to which products will sell more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e62c8",
   "metadata": {},
   "source": [
    "## üìö Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10907851",
   "metadata": {},
   "source": [
    "By the end of this session, you will be able to:\n",
    "\n",
    "- apply dimensionality reduction techniques to reduce features to a lower dimensional space\n",
    "- perform customer segmentation, determine optional number of clusters, and understand assumptions for used algorithm\n",
    "- understand what semi-supervised learning is and leverage it to improve performance of supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2869071e",
   "metadata": {},
   "source": [
    "## Task 1. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7505e",
   "metadata": {},
   "source": [
    "1. Load in the data. \n",
    "    \n",
    "    Import `pandas` as `pd` and use `pd.read_csv()` to read in `past.csv.gz` in the `dat` folder, saving it as `past`. \n",
    "    \n",
    "    Data in `past.csv.gz` was propcessed; e.g., features indicating time of day, day of week, month, and year of the purchase have been converted to one-hot representations of these categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e0876c",
   "metadata": {},
   "source": [
    "#### Finding out the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ac996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "# Out: /Users/shane/Documents/blog\n",
    "# Display all of the files found in your current working directory\n",
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ee4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import pandas as pd\n",
    "past = pd.read_csv('/home/krishanu_sinha/MLE-9/code/MLE-9/MLE-9/assignments/week-8-unsupervised-ML/dat/past.csv.gz')\n",
    "past.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85338a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "past.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21ec92",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Expected output </summary>\n",
    "\n",
    "```\n",
    "Index(['product_id', 'user_id', 'NumOfEventsInJourney', 'NumSessions',\n",
    "       'interactionTime', 'maxPrice', 'minPrice', 'NumCart', 'NumView',\n",
    "       'NumRemove', 'InsessionCart', 'InsessionView', 'InsessionRemove',\n",
    "       'Weekend', 'Fr', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed', '2019',\n",
    "       '2020', 'Jan', 'Feb', 'Oct', 'Nov', 'Dec', 'Afternoon', 'Dawn',\n",
    "       'EarlyMorning', 'Evening', 'Morning', 'Night', 'Purchased?', 'Noon',\n",
    "       'Category'],\n",
    "      dtype='object')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec9671a",
   "metadata": {},
   "source": [
    "2. What percentage of the interactions (rows) resulted in a purchase?\n",
    "\n",
    "    Do people mostly buy what they look at or do they do a lot of \"window shopping\" (shopping around without buying)?\n",
    "    \n",
    "    From the perspective of classification, is the data balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(past['Purchased?'])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d940e",
   "metadata": {},
   "source": [
    "3. Drop `product_id` and `user_id` and save the rest columns to a new `pd.DataFrame`:`X`; then pop the column `'Purchased?'` and save it to `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72fed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = past.drop(columns=['product_id','user_id'])\n",
    "y = X.pop('Purchased?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape == (5000, 34)\n",
    "assert y.shape == (5000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d487454",
   "metadata": {},
   "source": [
    "4. Apply [PCA (check documentation if unfamiliar)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the number of features down to **5**, save it to a numpy array named `X_reduced`. \n",
    "\n",
    "    Do you need to preprocess the data before performing PCA? Quick review [here: Importance of feature scaling](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html).\n",
    "    \n",
    "    If time permits, read [Does mean centering or feature scaling affect a Principal Component Analysis?](https://sebastianraschka.com/faq/docs/pca-scaling.html) or [discussion 1](https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c075852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=5, random_state=0, whiten=True)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ea4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_reduced.shape == (5000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23808b",
   "metadata": {},
   "source": [
    "5. Print out the percentage of variance explained by each of the selected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd43be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "#\n",
    "cum_sum_eigenvalues = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(cum_sum_eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c7c62",
   "metadata": {},
   "source": [
    "6. Review code in functions `visualize_2pcs` and `visualize_3pcs` below and visualize first few principal components in 2D and 3D plots, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900548a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_2pcs(pcs, y):\n",
    "    fig, ax = plt.subplots()\n",
    "    plot = plt.scatter(pcs[:,0], pcs[:,1], c=y) \n",
    "    ax.legend(\n",
    "        handles=plot.legend_elements()[0], \n",
    "        labels=['No', 'Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907db47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3pcs(pcs, y):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    plot = ax.scatter(pcs[:,0], pcs[:,1], pcs[:,2], c=y)\n",
    "    ax.legend(\n",
    "        handles=plot.legend_elements()[0], \n",
    "        labels=['No', 'Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94768a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_2pcs(X_reduced,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0dd5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_3pcs(X_reduced,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1e012",
   "metadata": {},
   "source": [
    "7. One way to assess the quality of the dimensionality reduction, when the groundtruth is available of course, is comparing the prediction performance using given features vs reduced (engineered) features.\n",
    "\n",
    "    Complete the wrapper function below that \n",
    "\n",
    "    - takes features, target, and a boolean parameter indicating whether to include standardization in the pipeline or not\n",
    "    - split the data into train (80%) and test (20%) datasets, set the random state for spliting at 0\n",
    "    - build a pipeline that \n",
    "\n",
    "        1) preprocessing data using standardization if the `standardize` is `True`; otherwise skip this step  \n",
    "\n",
    "        2) apply logistic regression ( are the labels balanced? )\n",
    "        \n",
    "    - fit the pipeline using training data\n",
    "    - print the classification report (use `sklearn.metrics.classification_report`) on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae830b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing basic packages\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Importing Sklearn module and classes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "#from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train(X, y, standardize = True) -> None:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "    sc = StandardScaler()\n",
    "    X_train_std = sc.fit_transform(X_train)\n",
    "    X_test_std = sc.fit_transform(X_test)\n",
    "    # Create an instance of LogisticRegression classifier\n",
    "    lr = LogisticRegression(C=100.0, random_state=1, solver='lbfgs', class_weight=\"balanced\")\n",
    "    #\n",
    "    # Fit the model\n",
    "    #\n",
    "    lr.fit(X_train_std, Y_train)\n",
    "    # Create the predictions\n",
    "    #\n",
    "    Y_predict = lr.predict(X_test_std)\n",
    "    return print(classification_report(Y_test, Y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e5df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b17ae17",
   "metadata": {},
   "source": [
    "Now apply the pipeline on the all the features `X` and review the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(X, y, standardize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c79f6",
   "metadata": {},
   "source": [
    "Similarly, apply the pipeline on the reduced / engineered features `X_reduced`. Should you include standardization in the pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(X_reduced,y, standardize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf1219",
   "metadata": {},
   "source": [
    "8. Are the results as expected? Discuss the pros and cons using reduced set of features in this application with your teammate. \n",
    "    *YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc9707",
   "metadata": {},
   "source": [
    "## Task 2. Customer Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1a95d",
   "metadata": {},
   "source": [
    "In this task, we apply k-means clustering on the reduced data, experimenting different vaules of `n_cluster`, summarize all this information in a single plot, the *Elbow* plot. In addition, leverage silhouette visualization to help decide the \"optimal\" number of clusters in our data and answer: \n",
    "\n",
    "1. Are there any patterns among customer purchasing behaviors?\n",
    "2. If so, what categories do they belong to? How do you characterize the clusters?\n",
    "3. If not, what followup steps and / or recommendations will you make as an MLE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6dc7fd",
   "metadata": {},
   "source": [
    "1. Look up the [documentation](https://scikit-learn.org/stable/modules/clustering.html) and import the model class for k-means from `sklearn.cluster`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbf216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4a915b",
   "metadata": {},
   "source": [
    "2. Complete `visualize_elbow`; inspect the code and complete\n",
    "\n",
    "    - fit k-means on the given data `X` and `k`, setting `random_state` to be 10 for reproducibility\n",
    "    - append the sum of squared distances of samples to their closest cluster center for each $k$ to list `inertias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2338ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_elbow(X, ks):\n",
    "    fig, ax = plt.subplots()\n",
    "    inertias = []\n",
    "    for k in ks:\n",
    "        kmeans = KMeans(n_clusters = k,     \n",
    "                    init = 'k-means++',                 # Initialization method for kmeans\n",
    "                    max_iter = 300,                     # Maximum number of iterations \n",
    "                    n_init = 10,                        # Choose how often algorithm will run with different centroid \n",
    "                    random_state = 10)                  # Choose random state for reproducibility\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    plt.plot(ks, inertias)\n",
    "    plt.xticks(ks)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc7db3",
   "metadata": {},
   "source": [
    "3. Visualize the elbow plot for the number of clusters ranging between 2 and 9. Discuss with your teammate, what is the 'optimal' number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea00376",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_elbow(X, range(2, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50183626",
   "metadata": {},
   "source": [
    "4. What are the disadvantage to use the Elbow method? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd270b30",
   "metadata": {},
   "source": [
    "#### Choosing  k manually from the elbow method can be erroneous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38e1e1",
   "metadata": {},
   "source": [
    "5. Let's try a different approach: [silhouette score](https://towardsdatascience.com/clustering-metrics-better-than-the-elbow-method-6926e1f723a6).\n",
    "\n",
    "    A helper function `visualize_silhouette` is provided for you (inspect the code in `utils.py`) and figure out how to use it to visualize k-means for k ranges from 2 to 8 on the reduced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_silhouette(8,X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf9e73",
   "metadata": {},
   "source": [
    "6. Instantiate a k-means model using the number of cluster that you deem optimal, assign it to `km`, and fit on the reduced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1947fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = 4,     \n",
    "                    init = 'k-means++',                 # Initialization method for kmeans\n",
    "                    max_iter = 300,                     # Maximum number of iterations \n",
    "                    n_init = 10,                        # Choose how often algorithm will run with different centroid \n",
    "                    random_state = 10)                  # Choose random state for reproducibility\n",
    "\n",
    "km.fit(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b802fa",
   "metadata": {},
   "source": [
    "7. What is the size of each cluster? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97447eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountFrequency(my_list):  \n",
    "    freq = {} \n",
    "    for item in my_list: \n",
    "        if (item in freq): \n",
    "            freq[item] += 1\n",
    "        else: \n",
    "            freq[item] = 1\n",
    "\n",
    "    for key, value in freq.items(): \n",
    "        print (\"% d : % d\"%(key, value)) \n",
    "        \n",
    "CountFrequency(km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf3260",
   "metadata": {},
   "source": [
    "8. Create a new column called `cluster_pca` in `past`, with values as predicted cluster index predicted by `km`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55328f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "past.loc[:,\"cluster_pca\"] = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ed713",
   "metadata": {},
   "outputs": [],
   "source": [
    "past.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea839f",
   "metadata": {},
   "source": [
    "9. Open ended: manipulate `past` and see if you can characterize each cluster (e.g., calculate statistics of / visualize features for each cluster), how will you intepret the results? \n",
    "\n",
    "    **Note**. This is probably the most important part as far as the business stakeholders are concerned: \"*What can I do with your results?*\" The math, modeling part is relatively easy, compared to actionable recommendations you make for business. Thus, before jumping on a different algorithm for the given task, do you best to 1) understand the data in depth 2) keep buisiness use cases in mind throughout all steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_0 = past[past['cluster_pca'] == 0]\n",
    "past_1 = past[past['cluster_pca'] == 1]\n",
    "past_2 = past[past['cluster_pca'] == 2]\n",
    "past_3 = past[past['cluster_pca'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autoviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04842369",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_0.to_csv('past_0.csv')\n",
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "\n",
    "#EDA using Autoviz\n",
    "autoviz = AutoViz_Class().AutoViz('past_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_1.to_csv('past_1.csv')\n",
    "#EDA using Autoviz\n",
    "autoviz = AutoViz_Class().AutoViz('past_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_2.to_csv('past_2.csv')\n",
    "#EDA using Autoviz\n",
    "autoviz = AutoViz_Class().AutoViz('past_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bf905",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_3.to_csv('past_3.csv')\n",
    "#EDA using Autoviz\n",
    "autoviz = AutoViz_Class().AutoViz('past_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c67cc",
   "metadata": {},
   "source": [
    "#### KEY TAKEAWAYS FROM EDA ON EACH CLUSTER:\n",
    "1. Median interaction Time is highest for past-3 cluster => 1 but that does not lead to a purchase as witnessed from the data.\n",
    "2. maxPrice is lowest for past-3 => 17.5\n",
    "3. Items Purchased for top 15 categories is lowest for Cluster-3\n",
    "\n",
    "NOTE: (minPrice is pretty close to normally distributed in all clusters and hence I have considered it for predictions)\n",
    "\n",
    "4. For cluster past-2,average minPrice by Purchased? often leads to a purchase but not in case of other clusters because past-2.\n",
    "5. cluster-2 mostly contains costly items as compared to the other items.\n",
    "6. similarly, for cluster past-2,average minPrice by Purchased? often does not leads to a purchase but not in case of other clusters because past-2 \n",
    "7. cluster mostly contains costly items as compared to the other items.\n",
    "\n",
    "8. Average Interaction time is high it leads to a Purchase in case of past_0, but not for past_1, past_2 and past_3 because there are too many Outliers in Past_0 cluster. Hence Median would have been a better metric in place of mean.\n",
    "9. Most Purchases for cluster past-3, happens only during the weekend (Saturday and Sunday).\n",
    "10. For past-3 cluster, in 2019, cluster there was more purchase as compared to 2020.\n",
    "11. For past-2 cluster, in 2019 and 2020, average purchase was almost same.\n",
    "12. For past-1 cluster, in 2020 cluster there was more purchase as compared to 2019.\n",
    "13. For past-0 cluster, in 2019, cluster there was more purchase as compared to 2020.\n",
    "14. ALL CLUSTER DISTRIBUTIONS ARE NOT GAUSSIAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045163e7",
   "metadata": {},
   "source": [
    "10. What are the assumptions for k-means? Judging by the cluster sizes, is k-means a good approach? \n",
    "\n",
    "    Scanning the list of [clustering algorithms](https://scikit-learn.org/stable/modules/clustering.html) implemented in scikit-learn, try at least one other algorithm, examine its assumptions, and intepret results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd3042",
   "metadata": {},
   "source": [
    "#### ASSUPTIONS OF KMEANS:\n",
    "1. k-means assume the variance of the distribution of each attribute (variable) is spherical;\n",
    "\n",
    "2. all variables have the same variance;\n",
    "\n",
    "3. the prior probability for all k clusters are the same, i.e. each cluster has roughly equal number of observations; If any one of these 3 assumptions is violated, then k-means will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99453a06",
   "metadata": {},
   "source": [
    "#### WHY KMEANS IS NOT A GOOD APPROACH?\n",
    "1. The distribution for all the three clusters weren't spherical or normally distributed.\n",
    "\n",
    "2. All variables don't have same variance.\n",
    "\n",
    "Hence, K-Means is not a good approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574709af",
   "metadata": {},
   "source": [
    "#### Since we have a high dimensional dataset hence my approach would be to reduce the dimensions using TSNE and then apply DBSCAN since DBSCAN cannot handle high dimensional dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2a615",
   "metadata": {},
   "source": [
    "#### What is TSNE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eabb35",
   "metadata": {},
   "source": [
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction technique used to represent high-dimensional dataset in a low-dimensional space of two or three dimensions so that we can visualize it. In contrast to other dimensionality reduction algorithms like PCA which simply maximizes the variance, t-SNE creates a reduced feature space where similar samples are modeled by nearby points and dissimilar samples are modeled by distant points with high probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf923e",
   "metadata": {},
   "source": [
    "#### Applying TSNE to reduce the dimensions and then visualize the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from numpy import reshape\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c31f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, random_state=123)\n",
    "z = tsne.fit_transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb68e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, random_state=123)\n",
    "z = tsne.fit_transform(X)\n",
    "df = pd.DataFrame()\n",
    "df[\"y\"] = y\n",
    "df[\"comp-1\"] = z[:,0]\n",
    "df[\"comp-2\"] = z[:,1]\n",
    "\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                data=df).set(title=\"New Products at Target Cosmetics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40edf25",
   "metadata": {},
   "source": [
    "#### ALTERNATE CLUSTERING TECHNIQUE: DBSCAN\n",
    "\n",
    "#### ASSUMPTIONS OF DBSCAN:\n",
    "1. Based on what I read on the internet, DBSCAN is a density-based clustering algorithm that works on the assumption that clusters are dense regions in space separated by regions of lower density. \n",
    "2. DBSCAN is a cool clustering algorithm that doesn't make assumptions about how data are distributed. \n",
    "\n",
    "#### Advantages of DBSCAN:\n",
    "1. Is great at separating clusters of high density versus clusters of low density within a given dataset.\n",
    "2. Is great with handling outliers within the dataset.\n",
    "\n",
    "#### Disadvantages of DBSCAN:\n",
    "\n",
    "1. While DBSCAN is great at separating high density clusters from low density clusters, DBSCAN struggles with clusters of similar density.\n",
    "2. Struggles with high dimensionality data. I know, this entire article I have stated how DBSCAN is great at contorting the data into different dimensions and shapes. However, DBSCAN can only go so far, if given data with too many dimensions, DBSCAN suffers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa56059",
   "metadata": {},
   "source": [
    "#### Since DBSCAN cannot handle too many dimensions hence we reduced our dataset to 2 dimensions and we will cluster our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b81d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "  \n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from kneed import KneeLocator\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors # importing the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and standardizing data\n",
    "X = StandardScaler().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4793ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['comp-1','comp-2']]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde32743",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.5, min_samples=10).fit(X)\n",
    "labels = db.labels_\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(X.iloc[:,0], X.iloc[:,1], hue=[\"cluster-{}\".format(x) for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc72d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.subplots_adjust(hspace=.5, wspace=.2)\n",
    "i = 1\n",
    "for x in range(10, 0, -1):\n",
    "    eps = 1/(11-x)\n",
    "    db = DBSCAN(eps=eps, min_samples=10).fit(X)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    \n",
    "    print(eps)\n",
    "    ax = fig.add_subplot(2, 5, i)\n",
    "    ax.text(1, 4, \"eps = {}\".format(round(eps, 1)), fontsize=25, ha=\"center\")\n",
    "    sns.scatterplot(X.iloc[:,0], X.iloc[:,1], hue=[\"cluster-{}\".format(x) for x in labels])\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1be129",
   "metadata": {},
   "source": [
    "#### We can see that we hit a sweet spot between eps=1.0. Our eps value must be far higher than this. eps values smaller than that have no information in them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522d135",
   "metadata": {},
   "source": [
    "#### A Systematic Method for Tuning the eps Value\n",
    "Since the eps figure is proportional to the expected number of neighbours discovered, we can use the nearest neighbours to reach a fair estimation for eps. Let us compute the nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=11)\n",
    "neighbors = nearest_neighbors.fit(X)\n",
    "distances, indices = neighbors.kneighbors(X)\n",
    "distances = np.sort(distances[:,10], axis=0)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.savefig(\"Distance_curve.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318df12",
   "metadata": {},
   "source": [
    "#### Locating our exact Knee point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d4e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "print(distances[knee.knee])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50b230",
   "metadata": {},
   "source": [
    "We can see that the detected knee point by this method is at distance 3.98. Now we can use this value as our eps to see how our new clustering would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0f29b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=distances[knee.knee], min_samples=10).fit(X)\n",
    "labels = db.labels_\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(X.iloc[:,0], X.iloc[:,1], hue=[\"cluster-{}\".format(x) for x in labels])\n",
    "plt.savefig(\"dbscan_with_knee.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18767fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f55c9ebd",
   "metadata": {},
   "source": [
    "11. Jot down recommendations or followup steps, detailing the reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a41aa0",
   "metadata": {},
   "source": [
    "Using TSNE to reduce the dimensions and using DBSCAN, we can clearly see the the cluster-0 is the most dense cluster. The other clusters are not as dense as cluster-0. We can therefore omit the other clusters and focus mostly on cluster-0 to find out about their purchase habbits and based on that we can formulate our business policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f44672",
   "metadata": {},
   "source": [
    "## Task 3. To launch or not to launch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e9fa9",
   "metadata": {},
   "source": [
    "In this task, we will work on a hypothetical application: cosmetics purchase prediction for new products with limited features. The intention here is to maximize **recall** so that no popular cosmetic is understocked. Overstocking is less of a concern since it will not cause disengagement in customers.\n",
    "\n",
    "The purchase status for each \"new\" product is known, but we only use the labels for benchmarking purpose. Instead, we use label spreading method to leverage a small amount of labeled data in combination with a larger amount of unlabeled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a4854",
   "metadata": {},
   "source": [
    "1. Read in the data in `new.csv.gz` and save it as a `pd.DataFrame` named `new`. This is the test dataset.\n",
    "\n",
    "    Look at the shape of `new` and inspect the frist few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import pandas as pd\n",
    "new = pd.read_csv('/home/krishanu_sinha/MLE-9/code/MLE-9/MLE-9/assignments/week-8-unsupervised-ML/dat/new.csv.gz')\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da7c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert new.shape == (30091, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6776b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7637957",
   "metadata": {},
   "source": [
    "2. How does the number of data points in the training set (`past`) compare to the number of datapoints in the test set (`new`)? \n",
    "\n",
    "    And how does the feature set in the training set compare to the feature set in the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "# computing number of rows\n",
    "rows = len(X_train.axes[0])\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefbd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814851b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1203/3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe739dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = len(new.axes[0])\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48955326",
   "metadata": {},
   "outputs": [],
   "source": [
    "new['Purchased?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce037906",
   "metadata": {},
   "outputs": [],
   "source": [
    "10359/30091"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff065f",
   "metadata": {},
   "source": [
    "#### In both the training set of past and new, 34% of cases have resulted in a puchase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223870d9",
   "metadata": {},
   "source": [
    "    *The number of datapoints in the training set is relatively small while the test set is quite large. The training set has more features than in the test set.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b229240",
   "metadata": {},
   "source": [
    "3. Are there any product ids in both the training and test datasets? Hint: use `np.intersect1d` or set operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981fe7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_prodid_arr = past[[\"product_id\"]].to_numpy()\n",
    "new_prodid_arr = new[[\"product_id\"]].to_numpy()\n",
    "\n",
    "intersecting_prod_ids = np.intersect1d(past_prodid_arr, new_prodid_arr)\n",
    "    \n",
    "print (intersecting_prod_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8890d8c",
   "metadata": {},
   "source": [
    "#### There are no common product ids in past (training) and new (test) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec1dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06769b43",
   "metadata": {},
   "source": [
    "4. What percentage of data points resulted in a purchase in the test set?\n",
    "\n",
    "    In reality, we won't be able to calculate information that is not available to you. Here, we simply demonstrated that the distributions in target between `past` and `new` are similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new['Purchased?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "10359/30091"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11414e22",
   "metadata": {},
   "source": [
    "#### In both the training set of past and new, 34% of cases have resulted in a puchase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae7223",
   "metadata": {},
   "source": [
    "5. Create `ndarray`s: `X_train`, `y_train`, `X_test`, and `y_test` according to the following guidelines.\n",
    "\n",
    "    - The `Purchased?` column is the target.\n",
    "    - `X_train` and `X_test` should contain the same features\n",
    "    - `product_id` should not be a feature.\n",
    "\n",
    "    Double check that the shapes of the four arrays are what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41017c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new.drop(columns=['product_id'])\n",
    "y = X.pop('Purchased?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5badbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape[0] == y_train.shape[0] # 5000\n",
    "assert X_train.shape[1] == X_test.shape[1]  # 3\n",
    "\n",
    "assert type(X_train) == np.ndarray # make sure you import numpy as np at this point\n",
    "assert type(X_train).__module__ == type(y_train).__module__ == np.__name__  # alternative way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd22e5",
   "metadata": {},
   "source": [
    "6. Let's fit a simple logistic regression on the training set (`X_train`, `y_train`) and report performance on the test set (`X_test`, `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e948fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(X, y, standardize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eea84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a805e6",
   "metadata": {},
   "source": [
    "7. Re-assemble data for semi-supervised learning. \n",
    "    - Use the features from the test set along with the features from the training set. \n",
    "    - Only use the labels from the training set but none from the test set.  \n",
    "    \n",
    "    Since we're using a large number of sampled features, but only a small number of these samples have labels, this is **semi-supervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa8a29",
   "metadata": {},
   "source": [
    "Create a matrix `X` that has the rows from `X_train` concatenated with the rows from `X_test`. Check the shape of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "past = past[['product_id', 'maxPrice', 'minPrice', 'Purchased?', 'Category']]\n",
    "X = pd.concat([past, new], sort=False)\n",
    "X.reset_index()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=['product_id','Purchased?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d4d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape == (35091, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dabc7c",
   "metadata": {},
   "source": [
    "Create the target array `y` by concatenating `y_train` with a vector of -1's, effectively creating a dummy label for the `X_test` rows in `X`. Check the shape of the array. It should have as many values as `X` has rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec0b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad0f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = [-1] * 14028\n",
    "vector = np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate((y_train, vector))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17318009",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import itemfreq\n",
    "itemfreq(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee27528c",
   "metadata": {},
   "source": [
    "8. Semi-supervised learning. \n",
    "\n",
    "    Scikit-learn provides two label propagation models: [`LabelPropagation`](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html) and [`LabelSpreading`](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html). Both work by constructing a similarity graph over all items in the input dataset. `LabelSpreading` is similar to the basic Label Propagation algorithm, but it uses an affinity matrix based on the normalized graph Laplacian and soft clamping across the labels; thus more robust to noise. We will be using scikit-learn's `LabelSpreading` model with `kNN`.\n",
    "    \n",
    "    Train a `LabelSpreading` model. Set `kernel` to `knn` and `alpha` to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelSpreading\n",
    "label_prop_model = LabelSpreading(kernel='knn',alpha=0.01)\n",
    "\n",
    "label_prop_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ee882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095f805",
   "metadata": {},
   "source": [
    "9. Extract the predictions for the test data. \n",
    "\n",
    "    You can get the predictions from the `transduction_` attribute. Note that there is a value for every row in `X`, so select just the values that correspond to `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc64f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_sup_preds = label_prop_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c71554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert semi_sup_preds.shape[0] == X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31745e2",
   "metadata": {},
   "source": [
    "10. Print the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c726511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, semi_sup_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, semi_sup_preds)) # make sure you properly import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a48ab",
   "metadata": {},
   "source": [
    "Let's bring the performance from the supervised learning model down to see the comparison; discuss the areas of improvement and reasons for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f32dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ee798",
   "metadata": {},
   "source": [
    "    *YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c972e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fbc9772",
   "metadata": {},
   "source": [
    "11. Read [Small Data Can Play a Big Role in AI](https://hbr.org/2020/02/small-data-can-play-a-big-role-in-ai) and discuss with your teammate about AI tools for training AI with small data and their use cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da2390",
   "metadata": {},
   "source": [
    "## Acknowledgement & References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67702bb8",
   "metadata": {},
   "source": [
    "- data was adapted from Kaggle: [eCommerce Events History in Cosmetics Shop](https://www.kaggle.com/mkechinov/ecommerce-events-history-in-cosmetics-shop)\n",
    "- function `visualize_silhouette` was adapted from [plot_kmeans_silhouette_analysis by scikit-learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "- [Categorizing Online Shopping Behavior from Cosmetics to Electronics: An Analytical Framework](https://arxiv.org/pdf/2010.02503.pdf)\n",
    "- [OPAM: Online Purchasing-behavior Analysis using Machine learning](https://arxiv.org/pdf/2102.01625.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "f70fe2e7e7cb52bf2bf0a2d8cc8af5768efe1556307d7c8f07dd0e6b20b16428"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
